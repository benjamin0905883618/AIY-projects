from __future__ import print_function

import collections
import io
import math

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from IPython  import display
from sklearn import metrics

tf.logging.set_verbosity(tf.logging.ERROR)
train_url = 'https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/train.tfrecord'
train_path = tf.keras.utils.get_file(train_url.split('/')[-1], train_url)
test_url = 'https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/test.tfrecord'
test_path = tf.keras.utils.get_file(test_url.split('/')[-1], test_url)
def _parse_function(record):
    features = {
        "terms": tf.VarLenFeature(dtypr=tf.string),
        "labels":tf.FixedLenFeature(shape=[1],dtype=tf.float32)
        }
    parsed_features = tf.parse_single_example(record,features)

    terms = parsed_features['terms'].values
    labels = parsed_features['labels']
    return {'terms':terms},labels
ds = tf.data.TFRecordDataset(train_path)
ds = ds.map(_parse_function)

n = ds.make_one_shot_iterator().get_next()
sess = tf.Session()
sess.run(n)

def _input_fn(input_filenames,numepochs = None,shuffle = True):
    ds = tf.data.TFRecordDataset(input_filenames)
    ds = ds.map(_parse_function)
    if shuffle:
        ds = ds.shuffle(10000)
        ds = ds.padded_batch(25,ds.output_shapes)
        ds = ds.repeat(num_epochs)
        features,labels = ds.make_one_shot_iterator().get_next()
        return features,labels
informative_terms = ("bad", "great", "best", "worst", "fun", "beautiful",
                     "excellent", "poor", "boring", "awful", "terrible",
                     "definitely", "perfect", "liked", "worse", "waste",
                     "entertaining", "loved", "unfortunately", "amazing",
                     "enjoyed", "favorite", "horrible", "brilliant", "highly",
                     "simple", "annoying", "today", "hilarious", "enjoyable",
                     "dull", "fantastic", "poorly", "fails", "disappointing",
                     "disappointment", "not", "him", "her", "good", "time",
                     "?", ".", "!", "movie", "film", "action", "comedy",
                     "drama", "family")
terms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key="terms",vocabulary_list=informative_terms)

my_optimizer = tf.train.AdagradOptimizer(learning_rate = 0.1)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer,5.0)

terms_embdeding_column = tf.feature_column.embedding_column(terms_feature_column,dimension = 2)
feature_column = [terms_embedding_column]

classifier = tf.estimator.DNNClassifier(
    feature_columns = [tf.feature_column.indicator_column(terms_feature_column)],
    hidden_units = [20,20],
    optimizer = my_optimizer
)
try:
    classifier.train(
        input_fn = lambda: _input_fn([train_path]),
        steps = 1000)
    evaluation_metrics = classifier.evaluate(
        input_fn = lambda: _input_fn([train_path]),
        steps = 1000)
    print("Training set metrics : ")
    for m in evaluation_metrics:
        print(m,evaluation_metrics[m])
    print("--------")

    evaluation_metrics = classifier.evaluate(
        input_fn = lambda: _input_fn(test_path),
        steps = 1000)
    for m in evaluation_metrics:
        print(m,evaluation_metrics[m])
    print("--------")
except ValueError as err:
    print(err)
classifier.get_variable_names()
classifier.get_variable_value('dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights').shape

import numpy as np
import matplotlib.pyplot as plt

embedding_matrix = classifier.get_variable_value('dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights')
for term_index in range(len(informative_terms)):
    term_vector = np.zeros(len(informative_terms))
    term_vector[term_index] = 1
    embedding_xy = np.matmul(term_vector,embedding_matrix)
    plt.text(embedding_xy[0],embedding_xy[1],informative_terms[term_index])

plt.rcParams["figure.figsize"] = (15,15)
plt.xlim(1.2*embedding_matrix.min(),1.2*embedding_matrix.max())
plt.ylim(1.2*embedding_matrix.min(),1.2*embedding_matrix.max())
plt.show()
                              
